{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- SETUP ---- #\n",
    "CHROMEDRIVER_PATH = \"/Users/parakhchaudhary/chromedriver_135/chromedriver-mac-x64/chromedriver\"\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Optional: remove to view browser\n",
    "service = Service(CHROMEDRIVER_PATH)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Visit the product catalog main page\n",
    "driver.get(\"https://www.shl.com/products/product-catalog/?type=1\")\n",
    "time.sleep(3)\n",
    "\n",
    "all_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _ in range(13):  # Go to page 14\n",
    "    next_buttons = driver.find_elements(By.XPATH, '//a[contains(text(), \"Next\")]')\n",
    "    if len(next_buttons) > 1:\n",
    "        driver.execute_script(\"arguments[0].click();\", next_buttons[1])  # second = individual\n",
    "    else:\n",
    "        driver.execute_script(\"arguments[0].click();\", next_buttons[0])\n",
    "    time.sleep(2)  # give content time to load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n",
      "Scraping page 21\n",
      "Scraping page 22\n",
      "Scraping page 23\n",
      "Scraping page 24\n",
      "Scraping page 25\n",
      "Scraping page 26\n",
      "Scraping page 27\n",
      "Scraping page 28\n",
      "Scraping page 29\n",
      "Scraping page 30\n",
      "Scraping page 31\n",
      "Scraping page 32\n",
      "Scraping page 33\n",
      "Scraping page 34\n",
      "Scraping page 35\n",
      "Scraping page 36\n",
      "Scraping page 37\n",
      "Scraping page 38\n",
      "Scraping page 39\n",
      "❌ No table found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seen_first_test_name = None\n",
    "\n",
    "page_num = 14\n",
    "while True:\n",
    "    print(f\"Scraping page {page_num}\")\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    tables = soup.find_all(\"table\")\n",
    "    if not tables:\n",
    "        print(\"❌ No table found.\")\n",
    "        break\n",
    "\n",
    "    table = tables[-1]  # Always get the last (individual test) table\n",
    "    rows = table.find_all(\"tr\")[1:]\n",
    "    if not rows:\n",
    "        break\n",
    "\n",
    "    # Check first row's test name to detect repetition\n",
    "    a_tag = rows[0].find_all(\"td\")[0].find(\"a\")\n",
    "    current_first_name = a_tag.text.strip()\n",
    "\n",
    "    if current_first_name == seen_first_test_name:\n",
    "        print(\"⚠️ Same content as previous page. Stopping.\")\n",
    "        break\n",
    "\n",
    "    seen_first_test_name = current_first_name\n",
    "\n",
    "    # Parse all rows on current page\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) < 4:\n",
    "            continue\n",
    "        a_tag = cols[0].find(\"a\")\n",
    "        name = a_tag.text.strip()\n",
    "        href = a_tag['href'].strip()\n",
    "        remote = \"Yes\"\n",
    "        adaptive = \"Yes\" if cols[2].find(\"svg\") else \"No\"\n",
    "        test_type = ''.join(span.text for span in cols[3].find_all(\"span\"))\n",
    "\n",
    "        all_data.append({\n",
    "            \"name\": name,\n",
    "            \"url\": href,\n",
    "            \"remote_testing\": remote,\n",
    "            \"adaptive_support\": adaptive,\n",
    "            \"test_type\": test_type\n",
    "        })\n",
    "\n",
    "    # Try clicking \"Next\"\n",
    "    try:\n",
    "        next_buttons = driver.find_elements(By.XPATH, '//a[contains(text(), \"Next\")]')\n",
    "\n",
    "        # Always pick the second one (index 1) on page 1\n",
    "        if len(next_buttons) > 1 and page_num == 1:\n",
    "            next_button = next_buttons[1]\n",
    "        else:\n",
    "            next_button = next_buttons[0]\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "\n",
    "        # Wait until first test name changes\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            lambda d: current_first_name not in d.page_source\n",
    "        )\n",
    "        page_num += 1\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print(\"✅ Reached last page.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Scraped 300 assessments and saved to shl_catalog3_31onwards.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- SAVE TO CSV ---- #\n",
    "output_file = \"shl_catalog3_31onwards.csv\"\n",
    "with open(output_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=all_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_data)\n",
    "\n",
    "print(f\"✅ Done. Scraped {len(all_data)} assessments and saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
