{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“˜ SHL Evaluation Notebook Template\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from recommender.embeddings import load_data, load_model\n",
    "from recommender.logic import recommend_tests\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Setup ----\n",
    "model = load_model()\n",
    "df, embeddings = load_data()\n",
    "\n",
    "# ---- Load Ground Truth from CSV ----\n",
    "test_df = pd.read_csv(\"dataset/Test_DATA.csv\")\n",
    "\n",
    "# Group by each query and collect expected assessments\n",
    "query_groups = test_df.groupby(\"Query\")[\"Assessments\"].apply(list).to_dict()\n",
    "\n",
    "# ---- Evaluation parameters ----\n",
    "K = 5  # evaluate top-K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for query, relevant_names in tqdm(query_groups.items()):\n",
    "    recs = recommend_tests(query, model, df, embeddings)\n",
    "    top_names = recs.head(K)[\"name\"].str.lower().tolist()\n",
    "\n",
    "    # Mark as relevant if exact match or substring match\n",
    "    y_true = [\n",
    "        1 if any(rel.lower() in name for rel in relevant_names) else 0\n",
    "        for name in top_names\n",
    "    ]\n",
    "\n",
    "    y_scores = recs.head(K)[\"similarity\"].tolist()\n",
    "\n",
    "    ap = average_precision_score(y_true, y_scores) if any(y_true) else 0.0\n",
    "    recall = sum(y_true) / len(relevant_names)\n",
    "    results.append({\"query\": query, \"AP@K\": ap, \"Recall@K\": recall})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "                                               query  AP@K  Recall@K\n",
      "0  Content Writer required, expert in English and...   0.0       0.0\n",
      "1  Development experience â€“ Java or JavaScript, C...   0.0       0.0\n",
      "2  Find me 1 hour long assesment for the below jo...   0.0       0.0\n",
      "3  I am hiring for Java developers who can also c...   0.0       0.0\n",
      "4  I am looking for a COO for my company in China...   0.0       0.0\n",
      "5  I want to hire new graduates for a sales role ...   0.0       0.0\n",
      "6  ICICI Bank Assistant Admin, Experience require...   0.0       0.0\n",
      "7  KEY RESPONSIBITILES:\\n \\n\\n Manage the sound-s...   0.0       0.0\n",
      "8  station through appropriate creative and marke...   0.0       0.0\n",
      "9  station through appropriate creative and marke...   0.0       0.0\n",
      "\n",
      "Mean Average Precision@5: 0.0000\n",
      "Mean Recall@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Results Summary ----\n",
    "metrics_df = pd.DataFrame(results)\n",
    "mean_ap = metrics_df[\"AP@K\"].mean()\n",
    "mean_recall = metrics_df[\"Recall@K\"].mean()\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(metrics_df)\n",
    "print(f\"\\nMean Average Precision@{K}: {mean_ap:.4f}\")\n",
    "print(f\"Mean Recall@{K}: {mean_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE TEST CASE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/k6kx692506v9dlxfqv8_0frm0000gn/T/ipykernel_26217/4068917725.py:17: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df[\"Query\"] = test_df[\"Query\"].fillna(method=\"ffill\")  # forward-fill missing queries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function load_model.<locals>.encode at 0x13dbb5620>\n",
      "\n",
      "Query: I am hiring for Java developers who can also collaborate effectively with my business teams. Looking for an assessment(s) that can be completed in 40 minutes.\n",
      "\n",
      "Ground Truth Assessments:\n",
      "- Automata - Fix (New) | SHL\n",
      "- Core Java (Entry Level) (New) \n",
      " | SHL\n",
      "- Java 8 (New) | SHL\n",
      "- Core Java (Advanced Level)  (New) | SHL\n",
      "- Agile Software Development \n",
      " | SHL\n",
      "- Technology Professional 8.0  Job Focused Assessment |  SHL\n",
      "- Computer Science (New) |  SHL\n",
      "\n",
      "Top 5 Recommendations:\n",
      "- Java Platform Enterprise Edition 7 (Java EE 7) (sim: 0.8445)\n",
      "- Java 2 Platform Enterprise Edition 1.4 Fundamental (sim: 0.8442)\n",
      "- Enterprise Java Beans (New) (sim: 0.8415)\n",
      "- Java Frameworks (New) (sim: 0.8407)\n",
      "- Java Web Services (New) (sim: 0.8398)\n",
      "\n",
      "AP@5: 0.0000\n",
      "Recall@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“˜ SHL Evaluation Notebook Template (Single Test Debug)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from recommender.embeddings import load_data, load_model\n",
    "from recommender.logic import recommend_tests\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# ---- Setup ----\n",
    "model = load_model()\n",
    "print(model)\n",
    "\n",
    "df, embeddings = load_data()\n",
    "\n",
    "# ---- Load Single Test Case from CSV ----\n",
    "test_df = pd.read_csv(\"dataset/Test_DATA.csv\")\n",
    "test_df[\"Query\"] = test_df[\"Query\"].fillna(method=\"ffill\")  # forward-fill missing queries\n",
    "\n",
    "# Choose first test case only\n",
    "single_query = test_df.iloc[0][\"Query\"]\n",
    "relevant_names = test_df[test_df[\"Query\"] == single_query][\"Assessments\"].dropna().tolist()\n",
    "\n",
    "print(f\"\\nQuery: {single_query}\")\n",
    "print(f\"\\nGround Truth Assessments:\")\n",
    "for a in relevant_names:\n",
    "    print(f\"- {a}\")\n",
    "\n",
    "# ---- Run Recommendation ----\n",
    "recs = recommend_tests(single_query, model, df, embeddings)\n",
    "top_recs = recs.head(5)\n",
    "\n",
    "print(\"\\nTop 5 Recommendations:\")\n",
    "for _, row in top_recs.iterrows():\n",
    "    print(f\"- {row['name']} (sim: {row['similarity']:.4f})\")\n",
    "\n",
    "# ---- Compute AP@K and Recall@K ----\n",
    "def normalize(text):\n",
    "    return text.lower().replace(\" | shl\", \"\").strip()\n",
    "\n",
    "top_names = top_recs[\"name\"].str.lower().tolist()\n",
    "y_true = [\n",
    "    1 if any(normalize(rel) in normalize(name) for rel in relevant_names) else 0\n",
    "    for name in top_names\n",
    "]\n",
    "y_scores = top_recs[\"similarity\"].tolist()\n",
    "\n",
    "ap = average_precision_score(y_true, y_scores) if any(y_true) else 0.0\n",
    "recall = sum(y_true) / len(relevant_names) if relevant_names else 0.0\n",
    "\n",
    "print(f\"\\nAP@5: {ap:.4f}\")\n",
    "print(f\"Recall@5: {recall:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
